作为一个严肃的可以操作的系统，你需要考虑这么几个方面：
1. 数据持久化(persistent) （历史数据的查询，以及服务器crash之后的重新恢复）
2. 可扩展性(extensibility)
3. 实时性(realtime) 与误差的trade off
4. 稳定性(stability)
5. 可用性(availability)
6. scalability - 这点存疑一下，因为需求并不明确

先考虑最原始但是可用的方法以及其中的几个关键点。

考虑到availability和stability，首先你不能有single point of failure。你要有一个分布式的队列服务，即使你的后端服务器crash，也不会丢失现在正在产生的数据。在你的服务器重新上线之后，可以继续处理在队列中累积的数据。这里的队列服务可用kafka或者kinesis.

persistent layer，因为我们这里先讲最原始的办法，我们只考虑用relational database。把所有data都存在一个地方显然是比较蠢的办法。比较理性的做法是按照IP hash，然后无论你是partition也好，还是干脆就分不同的table也好（因为IP的key空间是钉死的），把他们分到不同的服务器上去均衡负载。你的database server要有备份。要有slave，当master下线时slave仍然可以提供只读服务。

你的服务层要有redundancy, 一台机器因故下线了还可以有其它的顶住。

查询时从每个database查询top 10然后一个heap来merge result就好了。

上面这个解决方案是基本可行的，基于ip address的空间并不是很大，Oracle的db可以支持上G的table问题不大，你partition个四五次就还可以支持。

当然也有问题，比如说，写操作会很频繁，我们需要cache多次访问记录然后batch update，这里就涉及到了稳定性问题，因为你在什么地方cache这个中间结果，什么地方就有可能出问题，出了问题你就会丢数据。而在cache和真正写之间，也会有accuracy的问题。

所以如果要处理非常大的数据两，需要稍微fancy一点的解决方案。

比如说后端数据库，可以换用任何一种key-value store，或者推到极致，我们只需要raw访问记录。

然后将访问记录持久化到我们的分布式文件系统中，比如HDFS。这里能解决的是后端数据库的scale问题。我们需要周期性地在这个文件系统上跑Map-Reduce job，以获得更新的记录。然而要注意的是MR是非常慢的一个过程，通过MR sort算出来的结果往往是几个小时之前的了。Anyway到这个point，我们能够获得一个几小时前的访问频率的排名。

然后近几个小时中的访问频率，我们可以用apache storm来做stream处理，在服务层和之前MR预处理过的结果combine起来（一共才4G个条目，这里因为我们所有的历史结果都已经persistent，所以顶楼提到的内存处理方法是可行的——可以用数台机器，每台返回其上的前10，最后combine），得到最终的结果。


------------------
https://www.jiuzhang.com/qa/219/

这个题是一个很好的面试题，因为可以从算法和系统两个角度进行考察。

从算法的角度分析
从算法的角度，可以简单的称之为 Top K Frequent Elements in Recent X mins.
算法的角度，本质就是设计一个数据结构，支持给某个key的count+1（有一个post被分享了），给某个key的count-1（有一个分享的计数已经过期了），然后查询Top k。

做法是维护一个有序序列（用链表来维护），每个链表的节点的key是 count，value是list of elements that has this count，也用linked list串起来。 比如 a出现2次，b出现3次，c出现2次，d出现1次。那么这个链表就是：

{3: [b]} --> {2: [a ->c]} --> {1: [d]}
然后另外还需要一个hashmap，key是element，value是这个element在链表上的具体位置。
因为每一次的操作都是 count + 1和 count - 1，那么每次你通过 hashmap 找到对应的element在数据结构中的位置，+1的话，就是往头移动一格，-1的话，就是往尾巴移动一格。总而言之复杂度都是 O(1)。当你需要找 Top K 的时候，也是 O(k)的时间 可以解决的。

这个数据结构来自 LFU 的论文：
http://dhruvbird.com/lfu.pdf
LintCode上有LFU的题， 可以做一下：
www.lintcode.com/problem/lfu-cache

从系统设计的角度分析
所以一般来说，你可能首先需要按照 LFU 的思路答出上述的方法。这个就过了第一关，算法关。但是还没结束，这个题还有第二关，那就是系统设计关。上面的算法从算法的角度没办法更优了，每个分享操作都是O(1)的代价，每个求Top K都是O(k)的代价。已经很棒了。但是系统的角度出发，会存在这样一些问题：

如果QPS比较高，比如 1m，这个数据结构因为要加锁才能处理，所以会很慢。
分享的数据本身是分布式的，而不是中心化的，也就是说，比如有1000台web服务器，那么这1000台web服务器的是最先获得哪个帖子被分享的数据的，但是这些数据又都分布在这1000台web服务器中，如果用一个中心化的节点来做这个数据结构的服务，那么很显然这个中心节点会成为瓶颈。
比如这个系统用在twitter 这样的服务中，根据长尾理论，有80%或者更多的帖子连 Top 20% 都排不进去。而通常来说，从产品的角度，我们可能只需要知道 Top 20 最多是 Top 100 的数据就可以了。整个系统浪费了很多时间去统计那些永远不会成为Top 100的数据。
题目的要求是“5分钟，1小时，24小时”，而不是“最近2分零30秒”，“最近31秒”，也存在较大的优化空间
真实产品实时性要求和准确性没有那么高。你需要查询最近5分钟的Top K，结果得出的是最近5分02秒的Top K在产品中是没有太大问题的。
查询Top k 的次数远低于count + 1和 count -1 的次数。
综上所述我们给出一些针对性优化策略：

分布式统计 Distributed: 每隔5~10秒向中心节点汇报数据
也就是说，哪些帖子被分享了多少次这些数据，首先在 web server 中进行一次缓存，也就是说web server的一个进程接收到一个分享的请求之后，比如 tweet_id=100 的tweet被分享了。那么他把这个数据先汇报给web server上跑着的 agent 进程，这个agent进程在机器刚启动的时候，就会一直运行着，他接受在台web server上跑着的若干个web 进程(process) 发过来的 count +1 请求。

这个agent整理好这些数据之后，每隔5~10秒汇报给中心节点。这样子通过5~10s的数据延迟，解决了中心节点访问频率过高的问题。这个设计的思路在业界是非常常用的（做这种数据统计服务的都是这么做的），我们在《系统设计班》的datadog一节的课中，就讲到过用这种思路来统计每一个event发生了多少次。

分阶段统计 Level
在《系统设计班》的 ratelimiter 一节课中，我们也提到了这种分阶段统计的思想。即如果我要去算最近5分钟的数据，我就按照1秒钟为一个bucket的单位，收集最近300个buckets里的数据。如果是统计最近1小时的数据，那么就以1分钟为单位，收集最近60个Buckets的数据，如果是最近1天，那么就以小时为单位，收集最近24小时的数据。那么也就是说，当来了一个某个帖子被分享了1次的数据的时候，这条数据被会分别存放在当前时间(以秒为单位），当前分钟，当前小时的三个buckets里，用于服务之后最近5分钟，最近1小时和最近24小时的数据统计。

你可能会疑惑，为什么要这么做呢？这么做有什么好处呢？这样做的好处是，比如你统计最近1小时的数据的时候，就可以随着时间的推移，每次增加当前分钟的所有数据的统计，然后扔掉一小时里最早的1分钟里的所有数据。这样子就不用真的一个一个的+1或者-1了，而是整体的 +X 和 -X。当然，这样做之后，前面的算法部分提出来的数据结构就不work了，但是可以结合下面提到的数据抽样的方法，来减小所有候选 key 的数目，然后用普通的 Top K 的算法来解决问题。

参考练习题：http://www.lintcode.com/en/problem/top-k-frequent-words/

数据抽样 Sample
可以进行一定程度的抽样，因为那些Top K 的post，一定是被分享了很多很多次的，所以可以进行抽样记录。
如果是5分钟以内的数据，就不抽样，全记录。如果是最近1小时，就可以按照比如 1/100 的概率进行 sample。
这个思想我们在Web Crawler 的那节课中提到过。

缓存 Cache
对于最近5分钟的结果，每隔5s才更新一次。
对于最近1小时的结果，每隔1分钟更新一次。
对于最近24小时的结果，每隔10分钟才更新一次。

用户需要看结果的时候，永远看的是 Cache 里的结果。另外用一个进程按照上面的更新频率去逐渐更新Cache。

以上的这些优化方法，基本都基于一个基本原则：在很多的系统设计问题中，不需要做到绝对精确和绝对实时。特别是这种统计类的问题。如果你刷算法题刷很多，很容易陷入设计一个绝对精确和绝对实时的系统的误区。一般来说面试中不会这么要求，如果这么要求了，那说明考你的是算法，算法才需要绝对准确和实时。

总的来说，这道题考了算法，大数据和系统设计。Top K 的算法我们在九章算法班中讲过，Top K的大数据算法我们在大数据班中讲过，系统设计中用到的各类思想基本也都在课上讲过。所以好好听课很重要。
